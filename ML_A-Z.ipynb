{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the above libraries are imported\n"
     ]
    }
   ],
   "source": [
    "#[Course Name ::: Machine Learning A-Z ]\n",
    "#[Part 1 - Data Pre-processing]\n",
    "#[Downloading the dataset .... www.superdatascience.com/machine-learning]\n",
    "\n",
    "#[First Step - Importing the libraries]\n",
    "#[3 essential libraries --- Numpy,matplotlib,pandas]\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "print('All the above libraries are imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the dataset . The file is under the same directory as the conda root.\n",
    "dataset = pd.read_csv('Datasets//Part 1 - Data Preprocessing//Section 2 -------------------- Part 1 - Data Preprocessing --------------------//Data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we need to separate the dependant and the independant variables.\n",
    "\n",
    "x= dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,3].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The missing data >>> [[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 nan]\n",
      " [35.0 58000.0]\n",
      " [nan 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n",
      "The imputed dataset >>> [[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 63777.77777777778]\n",
      " [35.0 58000.0]\n",
      " [38.77777777777778 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Now , we need to take care of missing data.\n",
    "\n",
    "print(\"The missing data >>>\",x[:,1:3])\n",
    "\n",
    "# Most common idea is to replace it by the mean of the column.\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#we use the above library and take imputer class alone.\n",
    "# missing_values --> what values am i replacing .. i need to replace the values that are NaN in my dataset\n",
    "# strategy --> how am i imputing the missing values ... mean (default strategy)\n",
    "# axis --> axis = 0 (along columns) axis = 1 (along rows)\n",
    "imputer= Imputer(missing_values = 'NaN',strategy = 'mean',axis = 0)\n",
    "\n",
    "#Now we need to fit this imputer object into our dataset.\n",
    "# the missing values are in second and third column.Python has index from zero, but \n",
    "# while slicing the data the lower bound is included but the upper bound isn't. That is \n",
    "# why we have selected 1:3 and not 1:2 (this will select only the second column)\n",
    "imputer=imputer.fit(x[:,1:3])\n",
    "\n",
    "x[:,1:3] = imputer.transform(x[:,1:3])\n",
    "print(\"The imputed dataset >>>\",x[:,1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how the encoded values of the country column look like >>> [0 2 1 2 1 0 2 0 1 0]\n",
      "The features now looks like >>> [[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical data \n",
    "# In this dataset , the country data has 3 categories (Spain,France and Germany) \n",
    "# and the purchased has 2 categories(yes and no)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "labelEncoder_x = LabelEncoder()\n",
    "print(\"This is how the encoded values of the country column look like >>>\",labelEncoder_x.fit_transform(x[:,0]))\n",
    "x[:,0] = labelEncoder_x.fit_transform(x[:,0])\n",
    "\n",
    "# The problem that arises by encoding the categories is that, the machine learning algorithm thinks that \n",
    "# the countries with higher number carry higher value ..but that doesnt make any sense.\n",
    "# Hence , we create dummy columns .. \n",
    "\n",
    "one_hot_x = OneHotEncoder(categorical_features = [0]);\n",
    "x = one_hot_x.fit_transform(x).toarray()\n",
    "print(\"The features now looks like >>>\",x[:, 0:3])\n",
    "\n",
    "# For the dependant variable , there is no need to perform one hot encoding (or create dummy columns ) \n",
    "# because the machine learning algorithm will know that it is a category and there is no order between them.\n",
    "\n",
    "labelEncoder_y = LabelEncoder()\n",
    "y = labelEncoder_x.fit_transform(y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
