{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the above libraries are imported\n"
     ]
    }
   ],
   "source": [
    "#[Course Name ::: Machine Learning A-Z ]\n",
    "#[Part 1 - Data Pre-processing]\n",
    "#[Downloading the dataset .... www.superdatascience.com/machine-learning]\n",
    "\n",
    "#[First Step - Importing the libraries]\n",
    "#[3 essential libraries --- Numpy,matplotlib,pandas]\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "print('All the above libraries are imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the dataset . The file is under the same directory as the conda root.\n",
    "dataset = pd.read_csv('Datasets//Part 1 - Data Preprocessing//Section 2 -------------------- Part 1 - Data Preprocessing --------------------//Data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we need to separate the dependant and the independant variables.\n",
    "\n",
    "x= dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,3].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The missing data >>> [[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 nan]\n",
      " [35.0 58000.0]\n",
      " [nan 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n",
      "The imputed dataset >>> [[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 63777.77777777778]\n",
      " [35.0 58000.0]\n",
      " [38.77777777777778 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Now , we need to take care of missing data.\n",
    "\n",
    "print(\"The missing data >>>\",x[:,1:3])\n",
    "\n",
    "# Most common idea is to replace it by the mean of the column.\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#we use the above library and take imputer class alone.\n",
    "# missing_values --> what values am i replacing .. i need to replace the values that are NaN in my dataset\n",
    "# strategy --> how am i imputing the missing values ... mean (default strategy)\n",
    "# axis --> axis = 0 (along columns) axis = 1 (along rows)\n",
    "imputer= Imputer(missing_values = 'NaN',strategy = 'mean',axis = 0)\n",
    "\n",
    "#Now we need to fit this imputer object into our dataset.\n",
    "# the missing values are in second and third column.Python has index from zero, but \n",
    "# while slicing the data the lower bound is included but the upper bound isn't. That is \n",
    "# why we have selected 1:3 and not 1:2 (this will select only the second column)\n",
    "imputer=imputer.fit(x[:,1:3])\n",
    "\n",
    "x[:,1:3] = imputer.transform(x[:,1:3])\n",
    "print(\"The imputed dataset >>>\",x[:,1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how the encoded values of the country column look like >>> [0 2 1 2 1 0 2 0 1 0]\n",
      "The features now looks like >>> [[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical data \n",
    "# In this dataset , the country data has 3 categories (Spain,France and Germany) \n",
    "# and the purchased has 2 categories(yes and no)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "labelEncoder_x = LabelEncoder()\n",
    "print(\"This is how the encoded values of the country column look like >>>\",labelEncoder_x.fit_transform(x[:,0]))\n",
    "x[:,0] = labelEncoder_x.fit_transform(x[:,0])\n",
    "\n",
    "# The problem that arises by encoding the categories is that, the machine learning algorithm thinks that \n",
    "# the countries with higher number carry higher value ..but that doesnt make any sense.\n",
    "# Hence , we create dummy columns .. \n",
    "\n",
    "one_hot_x = OneHotEncoder(categorical_features = [0]);\n",
    "x = one_hot_x.fit_transform(x).toarray()\n",
    "print(\"The features now looks like >>>\",x[:, 0:3])\n",
    "\n",
    "# For the dependant variable , there is no need to perform one hot encoding (or create dummy columns ) \n",
    "# because the machine learning algorithm will know that it is a category and there is no order between them.\n",
    "\n",
    "labelEncoder_y = LabelEncoder()\n",
    "y = labelEncoder_x.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[Next step is to split the dataset into train and test data.]\n",
    "#[Reason --> When the ML algorithm learns something .. \n",
    "# we need to test the output against some data to verify its learning capabilities]\n",
    "from sklearn.cross_validation import  train_test_split\n",
    "x_train ,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scaled train data <<< [[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# The next step is Feature scaling ..\n",
    "# Most of the machine learning algorithms compute  euclidean distances between observations.\n",
    "# if the observations are not scaled along in the same range then the observation which has a higher number will dominate \n",
    "# other observations and the model will be skewed.\n",
    "# 2 types --> Standardisation and normalization\n",
    "# Standardisation --> x(i)-mean(x)/sd(x)\n",
    "# Normalization --> x(i)-min(x)/max(x)-min(x)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n",
    "print (\"The scaled train data <<<\",x_train)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "# Note : Some of the machine learning algorithms do take care of Feature scaling by itself.\n",
    "# For this course , a pre-processing template is created wherein only the loading of the dataset and the splitting \n",
    "# the data into a train and test is present. The data used in the course is pretty much clean and hence handling missing \n",
    "# data is not needed. The feature scaling may be needed for some algorithms."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
